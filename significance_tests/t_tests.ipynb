{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate effect sizes to quantify the magnitude of the differences\n",
    "# around 0.2 is a small effect, 0.5 a medium effect, 0.8 large effect\n",
    "# source https://machinelearningmastery.com/effect-size-measures-in-python/\n",
    "def cohen_d(dist_1, dist_2):\n",
    "    mean_diff = np.mean(dist_1) - np.mean(dist_2)\n",
    "    pooled_std = np.sqrt(\n",
    "        (np.std(dist_1, ddof = 1) ** 2 + \\\n",
    "         np.std(dist_2, ddof = 1) ** 2) / 2)\n",
    "    cohen_d = mean_diff / pooled_std \n",
    "    return cohen_d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo = pd.read_csv(r\"/path/to/metrics/folder\\NeMo_test_fixed.csv\")\n",
    "pyannote = pd.read_csv(r\"/path/to/metrics/folder\\PyAnnote_baseline_NEW.csv\")\n",
    "pyaudio = pd.read_csv(r\"/path/to/metrics/folder\\pyAudioAnalysis.csv\")\n",
    "sd = pd.read_csv(r\"/path/to/metrics/folder\\simple_diarizer.csv\")\n",
    "whisperx = pd.read_csv(r\"/path/to/metrics/folder\\WhisperX_baseline_NEW.csv\")\n",
    "\n",
    "df_TAM = pd.read_csv(r'\\research_data\\EDA\\This American Life\\TAM_Statistics.csv')\n",
    "df_RAMC = pd.read_csv(r'\\research_data\\EDA\\RAMC\\RAMC_Statistics.csv')\n",
    "df_AliFar = pd.read_csv(r'\\research_data\\EDA\\AliMeeting\\Ali_Far_Statistics.csv')\n",
    "df_AliNear = pd.read_csv(r'\\research_data\\EDA\\AliMeeting\\Ali_Near_Statistics.csv')\n",
    "df_AMI = pd.read_csv(r'\\research_data\\EDA\\AMI\\AMI_Statistics.csv')\n",
    "df_AI = pd.read_csv(r'\\research_data\\EDA\\AISHELL-4\\AISHELL_Statistics.csv')\n",
    "df_Earnings = pd.read_csv(r'\\research_data\\EDA\\Earnings-21\\Earnings_Statistics.csv')\n",
    "df_MSD = pd.read_csv(r'\\research_data\\EDA\\MSDWILD\\MSD_Statistics_new.csv')\n",
    "df_VOX = pd.read_csv(r'\\research_data\\EDA\\VoxConverse\\Vox_Statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_files = ['00856', '02218', '02901', '00143', '00501', '02025', '02677', '01392', '00801',\n",
    "  '03028', '00734', '01240', '00241', '01196', '01999', '02370', '01539', '02252',\n",
    "    '02837', '00804', '02020', '02891', '00084', '02802', '00995', '00645', '02388',\n",
    "      '00737', '02061', '02003', '00631', '02377', '02753', '00947', '03129', '02985',\n",
    "        '00785', '00144', '02641', '01229', '00198', '02995', '01009', '00378', '00239',\n",
    "          '01755', '00018', '02509', '01447', '02153', '02095', '01976', '01820', '01694',\n",
    "            '02868', '00927', '01458', '02737', '00678', '02209', '02904', '00616', '01954',\n",
    "              '03047', '00906', '02404', '02144', '00978', '01461', '00714', '01012', '00248',\n",
    "                '01616', '02646', '01955', '03020', '01668', '00990', '02399', '00417', '00820',\n",
    "                  '02790', '02341', '01793', '02584', '01649', '00471', '00913', '00877', '02644',\n",
    "                    '00025', '01768', '01997', '02709', '01935', '00769', '00170', '00430', '01219',\n",
    "                      '00919', '02260', '02289', '01067', '02162', '02752', '02913', '00663', '00841',\n",
    "                        '01580', '02311', '01151', '00600', '00939', '02441', '02258', '00443', '01112',\n",
    "                          '00935', '01744', '01039', '01554', '00429', '02951']\n",
    "df_MSD['File'] = df_MSD['File'].astype(str)\n",
    "df_MSD['File'] = df_MSD['File'].str.zfill(5)\n",
    "\n",
    "df_MSD = df_MSD[~df_MSD['File'].isin(h_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.concat([df_TAM, df_AliFar, df_RAMC, df_AliNear, df_AMI, df_AI, df_Earnings, df_MSD, df_VOX])\n",
    "df_base['File'] = df_base['File'].astype(\"str\")\n",
    "df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.loc[df_base[\"Dataset\"] == \"This American Life\", \"Dataset\"] = 'TAL'\n",
    "df_base.loc[df_base[\"Dataset\"] == \"MSDWild Few\", \"Dataset\"] = 'MSDWILD Few'\n",
    "df_base.loc[df_base[\"Dataset\"] == \"MSDWild Many\", \"Dataset\"] = 'MSDWILD Many'\n",
    "df_base.loc[df_base[\"Dataset\"] == \"Ali Far\", \"Dataset\"] = 'AliMeeting Far'\n",
    "df_base.loc[df_base[\"Dataset\"] == \"Ali Near\", \"Dataset\"] = 'AliMeeting Near'\n",
    "df_base.loc[df_base[\"Dataset\"] == \"Earnings\", \"Dataset\"] = 'Earnings-21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd['Specs'] = sd['Specs'].replace('ecapa_ahc', 'ECAPA + AHC')\n",
    "sd['Specs'] = sd['Specs'].replace('ecapa_sc', 'ECAPA + SC')\n",
    "sd['Specs'] = sd['Specs'].replace('xvec_ahc', 'x-vectors + AHC')\n",
    "sd['Specs'] = sd['Specs'].replace('xvec_sc', 'x-vectors + SC')\n",
    "pyannote['Method'] = pyannote['Method'].replace('PyAnnote', 'pyannote')\n",
    "\n",
    "nemo['Version'] = ''\n",
    "pyaudio['Version'] = ''\n",
    "sd['Version'] = ''\n",
    "nemo['Pipeline'] = 'NeMo'\n",
    "pyaudio['Pipeline'] = 'pyAudioAnalysis'\n",
    "sd['Pipeline'] = 'SD'\n",
    "sd['Method'] = sd['Specs']\n",
    "sd = sd.drop(['Specs'], axis=1)\n",
    "pyannote = pyannote.rename({'Method': 'Pipeline'}, axis=1)\n",
    "whisperx = whisperx.rename({'Method': 'Pipeline'}, axis=1)\n",
    "pyannote['Version'] = pyannote['Version'].astype(str)\n",
    "whisperx['Version'] = whisperx['Version'].astype(str)\n",
    "\n",
    "df_all = pd.concat([nemo, pyaudio, sd, pyannote, whisperx], axis=0)\n",
    "df_all['Dataset'] = df_all['Dataset'].replace('ThisAmericanLife', 'TAL')\n",
    "\n",
    "df_all['Set'] = df_all['Set'].fillna(\" \")\n",
    "df_all['Method'] = df_all['Method'].fillna(\" \")\n",
    "\n",
    "df_all['Set'] = df_all['Set'].str.replace('Test',' ')\n",
    "\n",
    "df_all['Value'] = round(df_all['Value'] * 100, 3)\n",
    "\n",
    "df_all['Pipeline Specs'] = df_all[['Pipeline', 'Version']].agg(' '.join, axis=1)\n",
    "df_all['Pipeline Specs'] = df_all[['Pipeline Specs', 'Method']].agg(''.join, axis=1)\n",
    "\n",
    "df_all['Pipeline Specs'] = df_all['Pipeline Specs'].astype(str)\n",
    "\n",
    "df_all['Metric'] = pd.Categorical(df_all['Metric'], \n",
    "                             ordered=True,\n",
    "                             categories=[\"DER\", \"JER\", \"Coverage\", \n",
    "                                         \"Purity\"])\n",
    "\n",
    "df_all['DataSet'] = df_all[['Dataset', 'Set']].agg(' '.join, axis=1)\n",
    "df_all['File'] = df_all['File'].astype(\"str\")\n",
    "\n",
    "df_all = df_all.drop(['Dataset', 'Set'], axis=1)\n",
    "df_all = df_all.rename(columns={\"DataSet\": \"Dataset\"})\n",
    "df_all = df_all.rename(columns={\"Pipeline Specs\": \"PipelineSpecs\"})\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.loc[df_all[\"Dataset\"] == \"TAL  \", \"Dataset\"] = 'TAL'\n",
    "df_all.loc[df_all[\"Dataset\"] == \"AISHELL-4  \", \"Dataset\"] = 'AISHELL-4'\n",
    "df_all.loc[df_all[\"Dataset\"] == \"VoxConverse  \", \"Dataset\"] = 'VoxConverse'\n",
    "df_all.loc[df_all[\"Dataset\"] == \"Earnings-21  \", \"Dataset\"] = 'Earnings-21'\n",
    "df_all.loc[df_all[\"Dataset\"] == \"RAMC  \", \"Dataset\"] = 'RAMC'\n",
    "df_all.loc[df_all[\"Dataset\"] == \"AMI  \", \"Dataset\"] = 'AMI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left = pd.merge(df_all, df_base, on=['File', 'Dataset'], how='left')\n",
    "merged_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left = merged_left[merged_left['Approach']=='Baseline']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_der = merged_left[merged_left['Metric'] == 'DER']\n",
    "df_jer = merged_left[merged_left['Metric'] == 'JER']\n",
    "df_cov = merged_left[merged_left['Metric'] == 'Coverage']\n",
    "df_pur = merged_left[merged_left['Metric'] == 'Purity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "alpha = 0.05\n",
    "\n",
    "def t_test(dist_1, dist_2):\n",
    "    _, p_value = ttest_ind(dist_1, dist_2)\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo_clust = merged_left[merged_left['Method']=='Clustering']\n",
    "nemo_neural = merged_left[merged_left['Method']=='Neural']\n",
    "nemo_joint = merged_left[merged_left['Method']=='Joint']\n",
    "nemo_joint_vad = merged_left[merged_left['Method']=='Joint (ASR-based TS)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### clustering vs neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(nemo_clust[nemo_clust['Metric'] == 'DER']['Value'], nemo_neural[nemo_neural['Metric'] == 'DER']['Value'])\n",
    "print(f\"T-test for DER - clustering vs neural:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(nemo_clust[nemo_clust['Metric'] == 'JER']['Value'], nemo_neural[nemo_neural['Metric'] == 'JER']['Value'])\n",
    "print(f\"T-test for JER - clustering vs neural:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(nemo_clust[nemo_clust['Metric'] == 'Coverage']['Value'], nemo_neural[nemo_neural['Metric'] == 'Coverage']['Value'])\n",
    "print(f\"T-test for Coverage - clustering vs neural:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(nemo_clust[nemo_clust['Metric'] == 'Purity']['Value'], nemo_neural[nemo_neural['Metric'] == 'Purity']['Value'])\n",
    "print(f\"T-test for Purity - clustering vs neural:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyannote baseline vs exact vs global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left = pd.merge(df_all, df_base, on=['File', 'Dataset'], how='left')\n",
    "pyannote_31 = merged_left[merged_left['PipelineSpecs']=='pyannote 3.1 ']\n",
    "\n",
    "baseline = pyannote_31[pyannote_31['Approach']=='Baseline']\n",
    "exact = pyannote_31[pyannote_31['Approach']=='Exact']\n",
    "global_ = pyannote_31[pyannote_31['Approach']=='Global']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline vs exact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p_value = t_test(baseline[baseline['Metric'] == 'DER']['Value'], exact[exact['Metric'] == 'DER']['Value'])\n",
    "print(f\"T-test for DER baseline vs exact:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(baseline[baseline['Metric'] == 'JER']['Value'], exact[exact['Metric'] == 'JER']['Value'])\n",
    "print(f\"T-test for JER baseline vs exact:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(baseline[baseline['Metric'] == 'Coverage']['Value'], exact[exact['Metric'] == 'Coverage']['Value'])\n",
    "print(f\"T-test for Coverage baseline vs exact:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(baseline[baseline['Metric'] == 'Purity']['Value'], exact[exact['Metric'] == 'Purity']['Value'])\n",
    "print(f\"T-test for Purity baseline vs exact:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline vs global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(baseline[baseline['Metric'] == 'DER']['Value'], global_[global_['Metric'] == 'DER']['Value'])\n",
    "print(f\"T-test for DER baseline vs global:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(baseline[baseline['Metric'] == 'JER']['Value'], global_[global_['Metric'] == 'JER']['Value'])\n",
    "print(f\"T-test for JER baseline vs global:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(baseline[baseline['Metric'] == 'Coverage']['Value'], global_[global_['Metric'] == 'Coverage']['Value'])\n",
    "print(f\"T-test for Coverage baseline vs global:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(baseline[baseline['Metric'] == 'Purity']['Value'], global_[global_['Metric'] == 'Purity']['Value'])\n",
    "print(f\"T-test for Purity baseline vs global:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exact vs global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(exact[exact['Metric'] == 'DER']['Value'], global_[global_['Metric'] == 'DER']['Value'])\n",
    "print(f\"T-test for DER exact vs global:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(exact[exact['Metric'] == 'JER']['Value'], global_[global_['Metric'] == 'JER']['Value'])\n",
    "print(f\"T-test for JER exact vs global:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(exact[exact['Metric'] == 'Coverage']['Value'], global_[global_['Metric'] == 'Coverage']['Value'])\n",
    "print(f\"T-test for Coverage exact vs global:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = t_test(exact[exact['Metric'] == 'Purity']['Value'], global_[global_['Metric'] == 'Purity']['Value'])\n",
    "print(f\"T-test for Purity exact vs global:\")\n",
    "print(f\"p-value: {np.round(p_value, 5)}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Statistically significant difference\\n\")\n",
    "else:\n",
    "    print(\"No statistically significant difference\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
