{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine as p9\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo = pd.read_csv(r\"\\research_data\\metrics\\NeMo.csv\")\n",
    "pyannote = pd.read_csv(r\"\\research_data\\metrics\\pyannote.csv\")\n",
    "pyaudio = pd.read_csv(r\"\\research_data\\metrics\\pyAudioAnalysis.csv\")\n",
    "sd = pd.read_csv(r\"\\research_data\\metrics\\simple_diarizer.csv\")\n",
    "whisperx = pd.read_csv(r\"\\research_data\\metrics\\WhisperX.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd['Specs'] = sd['Specs'].replace('ecapa_ahc', 'ECAPA + AHC')\n",
    "sd['Specs'] = sd['Specs'].replace('ecapa_sc', 'ECAPA + SC')\n",
    "sd['Specs'] = sd['Specs'].replace('xvec_ahc', 'x-vectors + AHC')\n",
    "sd['Specs'] = sd['Specs'].replace('xvec_sc', 'x-vectors + SC')\n",
    "pyannote['Method'] = pyannote['Method'].replace('PyAnnote', 'pyannote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nemo['Version'] = ''\n",
    "pyaudio['Version'] = ''\n",
    "sd['Version'] = ''\n",
    "nemo['Pipeline'] = 'NeMo'\n",
    "pyaudio['Pipeline'] = 'pyAudioAnalysis'\n",
    "sd['Pipeline'] = 'SD'\n",
    "sd['Method'] = sd['Specs']\n",
    "sd = sd.drop(['Specs'], axis=1)\n",
    "pyannote = pyannote.rename({'Method': 'Pipeline'}, axis=1)\n",
    "whisperx = whisperx.rename({'Method': 'Pipeline'}, axis=1)\n",
    "pyannote['Version'] = pyannote['Version'].astype(str)\n",
    "whisperx['Version'] = whisperx['Version'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([nemo, pyaudio, sd, pyannote, whisperx], axis=0)\n",
    "df_all['Dataset'] = df_all['Dataset'].replace('ThisAmericanLife', 'TAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Set'] = df_all['Set'].fillna(\" \")\n",
    "df_all['Method'] = df_all['Method'].fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Set'] = df_all['Set'].str.replace('Test',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.loc[df_all[\"Dataset\"] == \"This American Life\", \"Dataset\"] = 'TAL'\n",
    "df_all.loc[df_all[\"Dataset\"] == \"MSDWILD\", \"Dataset\"] = 'MSDWild'\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['DataSet'] = df_all[['Dataset', 'Set']].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Value'] = round(df_all['Value'] * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Pipeline Specs'] = df_all[['Pipeline', 'Version']].agg(' '.join, axis=1)\n",
    "df_all['Pipeline Specs'] = df_all[['Pipeline Specs', 'Method']].agg(''.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Pipeline Specs'] = df_all['Pipeline Specs'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Metric'] = pd.Categorical(df_all['Metric'], \n",
    "                             ordered=True,\n",
    "                             categories=[\"DER\", \"JER\", \"Coverage\", \n",
    "                                         \"Purity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[df_all['Approach']=='Baseline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyannote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_py = df_all[(df_all['Pipeline'] == 'pyannote') & ((df_all['DataSet'] == 'AliMeeting Near'))]\n",
    "df_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_py = df_all[(df_all['Pipeline'] == 'pyannote')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = p9.ggplot(df_py[df_py['Metric'].isin(['DER', 'JER']) ],\n",
    "  p9.aes(x = 'DataSet', y = 'Value', fill = 'Version')\n",
    "  ) + \\\n",
    "  p9.theme_bw() + \\\n",
    "  p9.geom_boxplot() + \\\n",
    "  p9.xlab('Data Set') + \\\n",
    "  p9.ylab('Value (%)') + \\\n",
    "  p9.scale_fill_manual(values=['#ef8a62', '#67a9cf'])  +\\\n",
    "  p9.facet_wrap(facets = '~Metric', ncol = 1, scales = 'free_y') + \\\n",
    "  p9.theme(\n",
    "    legend_position = 'top',\n",
    "    figure_size = (12, 8),\n",
    "\n",
    "    legend_box_margin = -10,   \n",
    "    axis_text_x  = p9.element_text(angle = 20, hjust = 15), \n",
    "    text = p9.element_text(size=12, weight='bold')\n",
    "  )\n",
    "plot\n",
    "save_file = f'\\\\research_data\\\\metrics\\\\DER_JER_pyannote.png'\n",
    "plot.save(filename = save_file, dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WhisperX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w = df_all[(df_all['Pipeline'] == 'WhisperX')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = p9.ggplot(df_w[df_w['Metric'].isin(['DER', 'JER']) ],\n",
    "  p9.aes(x = 'DataSet', y = 'Value', fill = 'Version')\n",
    "  ) + \\\n",
    "  p9.theme_bw() + \\\n",
    "  p9.geom_boxplot() + \\\n",
    "  p9.xlab('Data Set') + \\\n",
    "  p9.ylab('Value (%)') + \\\n",
    "  p9.scale_fill_manual(values=['#ef8a62', '#67a9cf'])  +\\\n",
    "  p9.facet_wrap(facets = '~Metric', ncol = 1, scales = 'free_y') + \\\n",
    "  p9.theme(\n",
    "    legend_position = 'top',\n",
    "    figure_size = (12, 8),\n",
    "\n",
    "    legend_box_margin = -10,   \n",
    "    axis_text_x  = p9.element_text(angle = 20, hjust = 15), \n",
    "    text = p9.element_text(size=12, weight='bold')\n",
    "  )\n",
    "plot\n",
    "save_file = f'\\\\research_data\\\\metrics\\\\DER_JER_WhisperX.png'\n",
    "plot.save(filename = save_file, dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple_diarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sd = df_all[(df_all['Pipeline'] == 'SD')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranked analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sd = df_all[(df_all['Pipeline'] == 'SD')]\n",
    "ranked_performance = df_sd[df_sd['Metric'].isin(['DER', 'JER'])][['DataSet', 'Method', 'File', 'Metric', 'Value']].copy()\n",
    "ranked_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_performance['rank'] = ranked_performance.groupby(by = ['DataSet', 'File', 'Metric'])['Value'].rank(method = 'min', ascending = True).astype(int)\n",
    "ranked_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rank = ranked_performance[['DataSet', 'Method', 'File', 'Metric', 'rank']] \\\n",
    "  .groupby(by = ['Method', 'Metric']).agg(rank_mean = ('rank', 'mean'), rank_sd = ('rank', 'std')).apply(lambda x : round(x, 3)).dropna()\n",
    "average_rank.reset_index(inplace = True)\n",
    "average_rank['label'] = average_rank.apply(lambda x : str(round(x['rank_mean'], 2)) + ' ± ' + str(round(x['rank_sd'], 2)), axis = 1)\n",
    "average_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_performance = df_sd[df_sd['Metric'].isin(['Coverage', 'Purity'])][['DataSet', 'Method', 'File', 'Metric', 'Value']].copy()\n",
    "ranked_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_performance['rank'] = ranked_performance.groupby(by = ['DataSet', 'File', 'Metric'])['Value'].rank(method = 'max', ascending = False).astype(int)\n",
    "ranked_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_performance[(ranked_performance['DataSet'] == 'AISHELL-4  ') & (ranked_performance['File'] == 'S_R004S01C01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rank = ranked_performance[['DataSet', 'Method', 'File', 'Metric', 'rank']] \\\n",
    "  .groupby(by = ['Method', 'Metric']).agg(rank_mean = ('rank', 'mean'), rank_sd = ('rank', 'std')).apply(lambda x : round(x, 3)).dropna()\n",
    "average_rank.reset_index(inplace = True)\n",
    "average_rank['label'] = average_rank.apply(lambda x : str(round(x['rank_mean'], 2)) + ' ± ' + str(round(x['rank_sd'], 2)), axis = 1)\n",
    "average_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nemo = df_all[(df_all['Pipeline'] == 'NeMo') & (df_all['Method'] != 'Joint') & (df_all['Method'] != 'Joint (ASR-based TS)')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_performance = df_nemo[df_nemo['Metric'].isin(['DER', 'JER'])][['DataSet', 'Method', 'File', 'Metric', 'Value']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_performance['rank'] = ranked_performance.groupby(by = ['DataSet', 'File', 'Metric'])['Value'].rank(method = 'min', ascending = True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rank = ranked_performance[['DataSet', 'Method', 'File', 'Metric', 'rank']] \\\n",
    "  .groupby(by = ['Method', 'Metric']).agg(rank_mean = ('rank', 'mean'), rank_sd = ('rank', 'std')).apply(lambda x : round(x, 3)).dropna()\n",
    "average_rank.reset_index(inplace = True)\n",
    "average_rank['label'] = average_rank.apply(lambda x : str(round(x['rank_mean'], 2)) + ' ± ' + str(round(x['rank_sd'], 2)), axis = 1)\n",
    "average_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_performance = df_nemo[df_nemo['Metric'].isin(['Coverage', 'Purity'])][['DataSet', 'Method', 'File', 'Metric', 'Value']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_performance['rank'] = ranked_performance.groupby(by = ['DataSet', 'File', 'Metric'])['Value'].rank(method = 'max', ascending = False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rank = ranked_performance[['DataSet', 'Method', 'File', 'Metric', 'rank']] \\\n",
    "  .groupby(by = ['Method', 'Metric']).agg(rank_mean = ('rank', 'mean'), rank_sd = ('rank', 'std')).apply(lambda x : round(x, 3)).dropna()\n",
    "average_rank.reset_index(inplace = True)\n",
    "average_rank['label'] = average_rank.apply(lambda x : str(round(x['rank_mean'], 2)) + ' ± ' + str(round(x['rank_sd'], 2)), axis = 1)\n",
    "average_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = ['AMI  ', 'AliMeeting Near', 'MSDWild Many', 'RAMC  ', 'TAL  ', 'VoxConverse  ']\n",
    "df_nemo_sub = df_nemo[df_nemo['DataSet'].isin(sets)]\n",
    "df_nemo_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_df = df_nemo_sub.groupby(by = ['DataSet', 'Method', 'Metric']).agg(Value_mean = ('Value', 'mean'), Value_sd = ('Value', 'std')).apply(lambda x : round(x, 3))\n",
    "average_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(average_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_comp = df_all[df_all['Pipeline Specs'].isin(['pyannote 3.1 ', 'WhisperX 3.1 ',\n",
    "                                             'NeMo Clustering', 'SD ECAPA + AHC',\n",
    "                                             'pyAudioAnalysis  ', 'NeMo Joint (ASR-based TS)'])]\n",
    "df_all_der = df_all_comp[(df_all_comp['Metric'] == 'DER')]\n",
    "df_all_comp.loc[df_all_comp[\"Pipeline Specs\"] == \"NeMo Joint (ASR-based TS)\", \"Pipeline Specs\"] = 'NeMo Joint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_performance = df_all_comp[df_all_comp['Metric'].isin(['DER', 'JER'])][['DataSet', 'Pipeline Specs', 'File', 'Metric', 'Value']].copy()\n",
    "ranked_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_performance['rank'] = ranked_performance.groupby(by = ['DataSet', 'File', 'Metric'])['Value'].rank(method = 'min', ascending = True).astype(int)\n",
    "ranked_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rank = ranked_performance[['DataSet', 'Pipeline Specs', 'File', 'Metric', 'rank']] \\\n",
    "  .groupby(by = ['Pipeline Specs', 'Metric']).agg(rank_mean = ('rank', 'mean'), rank_sd = ('rank', 'std')).apply(lambda x : round(x, 3)).dropna()\n",
    "average_rank.reset_index(inplace = True)\n",
    "average_rank['label'] = average_rank.apply(lambda x : str(round(x['rank_mean'], 2)) + ' ± ' + str(round(x['rank_sd'], 2)), axis = 1)\n",
    "average_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_performance = df_all_comp[df_all_comp['Metric'].isin(['Coverage', 'Purity'])][['DataSet', 'Pipeline Specs', 'File', 'Metric', 'Value']].copy()\n",
    "ranked_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_performance['rank'] = ranked_performance.groupby(by = ['DataSet', 'File', 'Metric'])['Value'].rank(method = 'max', ascending = False).astype(int)\n",
    "ranked_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rank = ranked_performance[['DataSet', 'Pipeline Specs', 'File', 'Metric', 'rank']] \\\n",
    "  .groupby(by = ['Pipeline Specs', 'Metric']).agg(rank_mean = ('rank', 'mean'), rank_sd = ('rank', 'std')).apply(lambda x : round(x, 3)).dropna()\n",
    "average_rank.reset_index(inplace = True)\n",
    "average_rank['label'] = average_rank.apply(lambda x : str(round(x['rank_mean'], 2)) + ' ± ' + str(round(x['rank_sd'], 2)), axis = 1)\n",
    "average_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composite metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeMo Clustering\n",
    "DER_mean = 2.86\n",
    "DER_std = 1.23\n",
    "JER_mean = 3.16\n",
    "JER_std = 1.40\n",
    "cov_mean = 2.60\n",
    "cov_std = 0.97\n",
    "pur_mean = 3.23\n",
    "pur_std = 1.54\n",
    "diarization_score_mean_clust = DER_mean * 0.25 +  JER_mean * 0.25 +  cov_mean * 0.25 +  pur_mean * 0.25\n",
    "diarization_score_std_clust = DER_std * 0.25 +  JER_std * 0.25 +  cov_std * 0.25 +  pur_std * 0.25\n",
    "print(\"NeMo Clustering\")\n",
    "print(diarization_score_mean_clust, \"+-\", diarization_score_std_clust)\n",
    "\n",
    "\n",
    "# NeMo Joint\n",
    "DER_mean = 3.32\n",
    "DER_std = 1.21\n",
    "JER_mean = 3.43\n",
    "JER_std = 1.31\n",
    "cov_mean = 2.23\n",
    "cov_std = 1.05\n",
    "pur_mean = 2.31\n",
    "pur_std = 1.68\n",
    "diarization_score_mean_joint = DER_mean * 0.25 +  JER_mean * 0.25 +  cov_mean * 0.25 +  pur_mean * 0.25\n",
    "diarization_score_std_joint = DER_std * 0.25 +  JER_std * 0.25 +  cov_std * 0.25 +  pur_std * 0.25\n",
    "print(\"\\nNeMo Joint\")\n",
    "print(diarization_score_mean_joint, \"+-\", diarization_score_std_joint)\n",
    "\n",
    "# SD ECAPA + AHC\n",
    "DER_mean = 3.29\n",
    "DER_std = 1.45\n",
    "JER_mean = 3.57\n",
    "JER_std = 1.57\n",
    "cov_mean = 2.96\n",
    "cov_std = 1.40\n",
    "pur_mean = 3.93\n",
    "pur_std = 1.59\n",
    "diarization_score_mean_sd = DER_mean * 0.25 +  JER_mean * 0.25 +  cov_mean * 0.25 +  pur_mean * 0.25\n",
    "diarization_score_std_sd = DER_std * 0.25 +  JER_std * 0.25 +  cov_std * 0.25 +  pur_std * 0.25\n",
    "print(\"\\nSD ECAPA + AHC\")\n",
    "print(diarization_score_mean_sd, \"+-\", diarization_score_std_sd)\n",
    "\n",
    "\n",
    "# pyannote 3.1\n",
    "DER_mean = 1.70\n",
    "DER_std = 1.12\n",
    "JER_mean = 1.63\n",
    "JER_std = 1.02\n",
    "cov_mean = 4.22\n",
    "cov_std = 0.97\n",
    "pur_mean = 3.70\n",
    "pur_std = 1.51\n",
    "diarization_score_mean_pya = DER_mean * 0.25 +  JER_mean * 0.25 +  cov_mean * 0.25 +  pur_mean * 0.25\n",
    "diarization_score_std_pya = DER_std * 0.25 +  JER_std * 0.25 +  cov_std * 0.25 +  pur_std * 0.25\n",
    "print(\"\\npyannote 3.1\")\n",
    "print(diarization_score_mean_pya, \"+-\", diarization_score_std_pya)\n",
    "\n",
    "# pyAudioAnalysis\n",
    "DER_mean = 5.69\n",
    "DER_std = 0.66\n",
    "JER_mean = 4.91\n",
    "JER_std = 1.42\n",
    "cov_mean = 5.84\n",
    "cov_std = 0.37\n",
    "pur_mean = 4.58\n",
    "pur_std = 1.58\n",
    "diarization_score_mean_pyAA = DER_mean * 0.25 +  JER_mean * 0.25 +  cov_mean * 0.25 +  pur_mean * 0.25\n",
    "diarization_score_std_pyAA = DER_std * 0.25 +  JER_std * 0.25 +  cov_std * 0.25 +  pur_std * 0.25\n",
    "print(\"\\npyAudioAnalysis\")\n",
    "print(diarization_score_mean_pyAA, \"+-\", diarization_score_std_pyAA)\n",
    "\n",
    "\n",
    "# WhisperX 3.1\n",
    "DER_mean = 3.92\n",
    "DER_std = 1.27\n",
    "JER_mean = 3.90\n",
    "JER_std = 1.38\n",
    "cov_mean = 3.23\n",
    "cov_std = 1.31\n",
    "pur_mean = 3.36\n",
    "pur_std = 1.94\n",
    "diarization_score_mean_w = DER_mean * 0.25 +  JER_mean * 0.25 +  cov_mean * 0.25 +  pur_mean * 0.25\n",
    "diarization_score_std_w = DER_std * 0.25 +  JER_std * 0.25 +  cov_std * 0.25 +  pur_std * 0.25\n",
    "print(\"\\nWhisperX 3.1\")\n",
    "print(diarization_score_mean_w, \"+-\", diarization_score_std_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyaud = df_all[(df_all['Pipeline'] == 'pyAudioAnalysis')]\n",
    "df_pyaud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = p9.ggplot(df_pyaud,\n",
    "  p9.aes(x = 'DataSet', y = 'Value', fill = 'Pipeline')\n",
    "  ) + \\\n",
    "  p9.theme_bw() + \\\n",
    "  p9.geom_boxplot() + \\\n",
    "  p9.xlab('Data Set') + \\\n",
    "  p9.ylab('Value (%)') + \\\n",
    "  p9.facet_wrap(facets = '~Metric', ncol = 1, scales = 'free_y') + \\\n",
    "  p9.scale_fill_manual(values=['#ef8a62'])  +\\\n",
    "  p9.theme(\n",
    "    legend_position = 'none',\n",
    "    figure_size = (8, 10),\n",
    "\n",
    "    legend_box_margin = -10,   \n",
    "    axis_text_x  = p9.element_text(angle = 30, hjust = 15), \n",
    "    text = p9.element_text(size=12, weight='bold')\n",
    "  )\n",
    "plot\n",
    "save_file = f'\\\\research_data\\\\metrics\\\\pyaud.png'\n",
    "plot.save(filename = save_file, dpi = 300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
